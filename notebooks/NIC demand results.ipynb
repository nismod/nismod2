{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect NISMOD2 results for NIC resilience - demand scenarios\n",
    "\n",
    "- water demand\n",
    "- energy demand\n",
    "- transport OD matrix, trip distribution, energy consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas\n",
    "import geopandas\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_demand_files = glob.glob(\"../results/nic_w*/water_demand/decision_0/*.csv\")\n",
    "dfs = []\n",
    "for fn in water_demand_files:\n",
    "    demand_scenario = re.search(\"__(\\w+)\", fn).group(1)\n",
    "    year = re.search(\"2\\d+\", fn).group(0)\n",
    "    df = pandas.read_csv(fn, dtype={\n",
    "        'water_resource_zones': 'category'\n",
    "    })\n",
    "    df['timestep'] = int(year)    \n",
    "    df.timestep = df.timestep.astype('int16')\n",
    "    df['demand_scenario'] = demand_scenario\n",
    "    df.demand_scenario = df.demand_scenario.astype(CategoricalDtype(['BL', 'FP']))\n",
    "    dfs.append(df)\n",
    "\n",
    "water_demand = pandas.concat(dfs)\n",
    "del dfs\n",
    "water_demand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_demand.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_demand.to_parquet('nic_water_demand.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_demand_files = glob.glob(\"../results/nic_ed/energy_demand_constrained/decision_0/*2050.csv\")\n",
    "dfs = []\n",
    "for n, fn in enumerate(tqdm(energy_demand_files)):\n",
    "    output = re.search(\"output_(\\w+)_timestep\", fn).group(1)\n",
    "    year = re.search(\"2\\d+\", fn).group(0)\n",
    "    sector = re.match(\"[^_]*\", output).group(0)\n",
    "    service = output.replace(sector + \"_\", \"\")\n",
    "    fuel = re.match(\"hydrogen|oil|solid_fuel|gas|electricity|biomass\", service).group(0)\n",
    "    service = service.replace(fuel + \"_\", \"\")\n",
    "    service = service.replace(\"_\" + fuel, \"\")\n",
    "    service = service.replace(\"_CHP\", \"\")\n",
    "    service = service.replace(\"_fuel_cell\", \"\")\n",
    "    df = pandas.read_csv(fn, dtype={\n",
    "        'hourly': 'int16',\n",
    "        'lad_uk_2016': 'category'\n",
    "    }).rename(columns={\n",
    "        output: 'energy_demand'\n",
    "    })\n",
    "    df['timestep'] = int(year)\n",
    "    df.timestep = df.timestep.astype('int16')\n",
    "    df['fuel'] = fuel\n",
    "    df.fuel = df.fuel.astype(CategoricalDtype(['hydrogen', 'oil', 'solid_fuel', 'gas', 'electricity', 'biomass']))\n",
    "    df['service'] = service\n",
    "    df.service = df.service.astype(CategoricalDtype(['boiler', 'district_heating', 'non_heating', 'heat_pumps', 'fuel_cell']))\n",
    "    df['sector'] = sector\n",
    "    df.sector = df.sector.astype(CategoricalDtype(['industry', 'service', 'residential']))\n",
    "    dfs.append(df)\n",
    "energy_demand = pandas.concat(dfs)\n",
    "del dfs\n",
    "energy_demand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_demand.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_demand.to_parquet('nic_energy_demand.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transport energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_to_int(h):\n",
    "    \"\"\"Convert from string-named hours to 24-hour clock integers\n",
    "    \"\"\"\n",
    "    lu = {\n",
    "        'MIDNIGHT': 0,\n",
    "        'ONEAM': 1,\n",
    "        'TWOAM': 2,\n",
    "        'THREEAM': 3,\n",
    "        'FOURAM': 4,\n",
    "        'FIVEAM': 5,\n",
    "        'SIXAM': 6,\n",
    "        'SEVENAM': 7,\n",
    "        'EIGHTAM': 8,\n",
    "        'NINEAM': 9,\n",
    "        'TENAM': 10,\n",
    "        'ELEVENAM': 12,\n",
    "        'NOON': 11,\n",
    "        'ONEPM': 13,\n",
    "        'TWOPM': 14,\n",
    "        'THREEPM': 15,\n",
    "        'FOURPM': 16,\n",
    "        'FIVEPM': 17,\n",
    "        'SIXPM': 18,\n",
    "        'SEVENPM': 19,\n",
    "        'EIGHTPM': 20,\n",
    "        'NINEPM': 21,\n",
    "        'TENPM': 22,\n",
    "        'ELEVENPM': 23,\n",
    "    }\n",
    "    return lu[h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_paths = glob.glob(\"../results/nic_ed_tr/transport/decision_0/*vehicle*\")\n",
    "dfs = []\n",
    "for fn in ev_paths:\n",
    "    output = re.search(\"output_(\\w+)_timestep\", fn).group(1)\n",
    "    year = re.search(\"2\\d+\", fn).group(0)\n",
    "    df = pandas.read_parquet(fn).rename(columns={\n",
    "        output: 'value'\n",
    "    })\n",
    "    df['timestep'] = int(year)\n",
    "    df['key'] = output\n",
    "    dfs.append(df)\n",
    "    \n",
    "ev_demand = pandas.concat(dfs) \\\n",
    "    .reset_index()\n",
    "del dfs\n",
    "\n",
    "ev_demand.annual_day_hours = ev_demand.annual_day_hours.apply(hours_to_int)\n",
    "ev_demand = ev_demand \\\n",
    "    .pivot_table(\n",
    "        index=['timestep', 'lad_gb_2016', 'annual_day_hours'], \n",
    "        columns='key', \n",
    "        values='value'\n",
    "    ) \\\n",
    "    .reset_index()\n",
    "\n",
    "del ev_demand.columns.name\n",
    "\n",
    "ev_demand.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_demand.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_demand.to_parquet('nic_ev_demand.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transport trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data_path = \"../results/nic_ed_tr/transport-raw_data_results_nic_ed_tr/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015 estimated tempro OD\n",
    "tempro15 = pandas.read_csv(tr_data_path + \"data/csvfiles/temproMatrixListBased198WithMinor4.csv\")\n",
    "tempro15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015 aggregated LAD OD\n",
    "lad15 = pandas.read_csv(tr_data_path + \"data/csvfiles/ladFromTempro198ODMWithMinor4.csv\") \\\n",
    "    .sort_values(by=['origin', 'destination'])\n",
    "lad15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2050 predicted LAD OD - to disaggregate\n",
    "lad50 = pandas.read_csv(tr_data_path + \"output/2050/predictedODMatrix.csv\") \\\n",
    "    .melt(id_vars='origin', var_name='destination', value_name='flow') \\\n",
    "    .sort_values(by=['origin', 'destination'])\n",
    "lad50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempro zones shapefile - with LAD codes already attached\n",
    "tempro_lad = geopandas.read_file(tr_data_path + \"data/shapefiles/tempro2.shp\") \\\n",
    "    .rename(columns={\n",
    "        'Zone_Name': 'tempro_name',\n",
    "        'Zone_Code': 'tempro',\n",
    "        'LAD_Code': 'lad',\n",
    "        'Local_Auth': 'lad_name'\n",
    "    }) \\\n",
    "    [['lad', 'lad_name', 'tempro', 'tempro_name']] \\\n",
    "    .sort_values(by=['lad', 'tempro'])\n",
    "tempro_lad_codes = tempro_lad[['lad', 'tempro']]\n",
    "tempro_lad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with tempro 2015 OD\n",
    "\n",
    "# merge on LAD codes for tempro origins\n",
    "df = tempro15 \\\n",
    "    .rename(columns={'flow': 'tempro2015'}) \\\n",
    "    .merge(tempro_lad_codes, left_on='origin', right_on='tempro') \\\n",
    "    .drop(columns='tempro') \\\n",
    "    .rename(columns={'lad': 'origin_lad'})\n",
    "\n",
    "# merge on LAD codes for tempro destinations\n",
    "df = df \\\n",
    "    .merge(tempro_lad_codes, left_on='destination', right_on='tempro') \\\n",
    "    .drop(columns='tempro') \\\n",
    "    .rename(columns={'lad': 'destination_lad'})\n",
    "\n",
    "# merge on LAD 2015 flows\n",
    "df = df \\\n",
    "    .merge(lad15, left_on=['origin_lad', 'destination_lad'], right_on=['origin', 'destination'], suffixes=('', '_y')) \\\n",
    "    .drop(columns=['origin_y', 'destination_y']) \\\n",
    "    .rename(columns={'flow': 'lad2015'})\n",
    "\n",
    "# merge on LAD 2050 flows\n",
    "df = df \\\n",
    "    .merge(lad50, left_on=['origin_lad', 'destination_lad'], right_on=['origin', 'destination'], suffixes=('', '_y')) \\\n",
    "    .drop(columns=['origin_y', 'destination_y']) \\\n",
    "    .rename(columns={'flow': 'lad2050'})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disaggregation calculation\n",
    "df['tempro2050'] = (df.tempro2015 * (df.lad2050 / df.lad2015)) \\\n",
    "    .round() \\\n",
    "    .astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check\n",
    "df[(df.origin_lad == 'E09000007') & (df.destination_lad == 'E09000029')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['lad2015', 'lad2050', 'origin_lad', 'destination_lad'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('nic_transport_trips.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
